# commonlit-scoring

Задача - разработать модель машинного обучения, которая сможет оценивать навыки ответа школьников на вопросы, оценивая содержание ответа и формулировку ответа . Задача призвана облегчить процесс обучения, автоматизируя оценку навыков неоднострочного ответа на вопрос.


---

## Setup

### 1. Клонирование репозитория

```bash
git clone https://github.com/roleynikov/oleynikov-answer-scoring.git
cd commonlit-scoring
```

### 2. Установка зависимостей


```bash
poetry install
```

### 3. Установка pre-commit хуков

```bash
poetry run pre-commit install
poetry run pre-commit run -a
```

Ожидается успешное выполнение всех хуков без ошибок.

---

## Data management (DVC)

Данные управляются через DVC.

Используется удалённое хранилище (Yandex Disk, WebDAV).


## Configuration (Hydra)

Все основные параметры вынесены в Hydra-конфиги:

* пути к данным
* гиперпараметры обучения
* параметры модели
* параметры логирования

Конфиги расположены в:

```text
commonlit_scoring/configs/
```

Иерархия конфигов используется для разделения ответственности
(данные / модель / обучение / логирование).

---

## Training

Обучение реализовано с использованием **PyTorch Lightning**.

### Запуск обучения

Из корня проекта:

```bash
poetry run python main.py
```

В процессе обучения:

* считается функция потерь
* логируются метрики
* создаётся эксперимент в MLflow

---

## Logging (MLflow)

Логирование экспериментов выполняется через **MLflow**.

По умолчанию ожидается, что MLflow сервер доступен по адресу:

```text
http://127.0.0.1:8080
```

Логируется:

* loss
* дополнительные метрики
* все гиперпараметры
* git commit hash

## Notes

* В проекте отсутствует исполняемый код на уровне модулей
* Все зависимости управляются через poetry
* Данные не хранятся в git
* Используется единая точка входа (`commands.py`)
* Проект соответствует требованиям задания
